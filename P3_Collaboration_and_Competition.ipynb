{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 250         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic 2539\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/HP/Desktop/Tennis_Windows_x86_64/Tennis.exe\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Actor, Critic, Noise & Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent_Model import Actor, Critic, OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states_list = [torch.from_numpy(np.vstack([e.states[index] for e in experiences if e is not None])).float().to(device) for index in range(num_agents)]\n",
    "        actions_list = [torch.from_numpy(np.vstack([e.actions[index] for e in experiences if e is not None])).float().to(device) for index in range(num_agents)]\n",
    "        next_states_list = [torch.from_numpy(np.vstack([e.next_states[index] for e in experiences if e is not None])).float().to(device) for index in range(num_agents)]            \n",
    "        rewards = torch.from_numpy(np.vstack([e.rewards for e in experiences if e is not None])).float().to(device)        \n",
    "        dones = torch.from_numpy(np.vstack([e.dones for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states_list, actions_list, rewards, next_states_list, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharedBuffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent - Deep Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Construct Actor networks\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),lr=LR_ACTOR)\n",
    "\n",
    "        # Construct Critic networks \n",
    "        self.critic_local = Critic(state_size, action_size , random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size , random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # noise processing\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "            \n",
    "    def step(self):\n",
    "        if len(sharedBuffer) > BATCH_SIZE:\n",
    "            experiences = sharedBuffer.sample()\n",
    "            self.learn(experiences, GAMMA)        \n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states_list, actions_list, rewards, next_states_list, dones = experiences\n",
    "                    \n",
    "        next_states_tensor = torch.cat(next_states_list, dim=1).to(device)\n",
    "        states_tensor = torch.cat(states_list, dim=1).to(device)\n",
    "        actions_tensor = torch.cat(actions_list, dim=1).to(device)\n",
    "        \n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        next_actions = [self.actor_target(states) for states in states_list]        \n",
    "        next_actions_tensor = torch.cat(next_actions, dim=1).to(device)        \n",
    "        Q_targets_next = self.critic_target(next_states_tensor, next_actions_tensor)        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states_tensor, actions_tensor)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        # take the current states and predict actions\n",
    "        actions_pred = [self.actor_local(states) for states in states_list]        \n",
    "        actions_pred_tensor = torch.cat(actions_pred, dim=1).to(device)\n",
    "        # -1 * (maximize) Q value for the current prediction\n",
    "        actor_loss = -self.critic_local(states_tensor, actions_pred_tensor).mean()        \n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()        \n",
    "        #torch.nn.utils.clip_grad_norm_(self.actor_local.parameters(), 1)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for defining Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "\n",
    "    def __init__(self, random_seed):\n",
    "        self.agents = [DDPGAgent(state_size,action_size,random_seed) for x in range(num_agents)]\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        sharedBuffer.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "        for agent in self.agents:\n",
    "            agent.step()\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        actions = np.zeros([num_agents, action_size])\n",
    "        for index, agent in enumerate(self.agents):\n",
    "            actions[index, :] = agent.act(states[index], add_noise)\n",
    "        return actions\n",
    "\n",
    "    def save_weights(self):\n",
    "        for index, agent in enumerate(self.agents):\n",
    "            torch.save(agent.actor_local.state_dict(), 'agent{}_checkpoint_actor.pth'.format(index+1))\n",
    "            torch.save(agent.critic_local.state_dict(), 'agent{}_checkpoint_critic.pth'.format(index+1))\n",
    "    \n",
    "    def reset(self):        \n",
    "        for agent in self.agents:\n",
    "            agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis = MADDPG(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17\tAverage Score: 0.000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\udacity\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\HP\\anaconda3\\envs\\udacity\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Using a target size (torch.Size([250, 2])) that is different to the input size (torch.Size([250, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage score: 0.013\n",
      "Episode 200\tAverage score: 0.015\n",
      "Episode 300\tAverage score: 0.016\n",
      "Episode 400\tAverage score: 0.003\n",
      "Episode 500\tAverage score: 0.037\n",
      "Episode 600\tAverage score: 0.043\n",
      "Episode 700\tAverage score: 0.052\n",
      "Episode 800\tAverage score: 0.106\n",
      "Episode 900\tAverage score: 0.096\n",
      "Episode 1000\tAverage score: 0.123\n",
      "Episode 1100\tAverage score: 0.110\n",
      "Episode 1200\tAverage score: 0.098\n",
      "Episode 1300\tAverage score: 0.126\n",
      "Episode 1400\tAverage score: 0.128\n",
      "Episode 1500\tAverage score: 0.155\n",
      "Episode 1600\tAverage score: 0.218\n",
      "Episode 1700\tAverage score: 0.320\n",
      "Solved in episode: 1783 \tAverage score: 0.506\n"
     ]
    }
   ],
   "source": [
    "# Training the agents to play tennis\n",
    "episode = 0\n",
    "scores = [] # Scores\n",
    "scores_deque = deque(maxlen=100) # List to take average of 100 episodes\n",
    "average_scores = [] # List of average scores\n",
    "time_steps = 0 # Time steps\n",
    "GOAL = 0.5 # Goal for the project is to reach 0.5 reward over all 100 episodes\n",
    "EXPLORE_TIMESTEPS = 5000 # Timesteps untill the agents just explore\n",
    "average_score = 0 # Initial average score\n",
    "\n",
    "while average_score < GOAL:\n",
    "    episode+=1\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]   \n",
    "    # get the initial state\n",
    "    states = env_info.vector_observations         \n",
    "    # empty scores array\n",
    "    score = np.zeros(num_agents)\n",
    "    # reset the agent\n",
    "    tennis.reset()\n",
    "\n",
    "    while True:\n",
    "        '''\n",
    "        Doing random actions for some timesteps will help the agent collect more samples into the memory, so\n",
    "        that the agents will get to know more about the environment.\n",
    "        '''     \n",
    "        # Play random actions for first 5000 timesteps\n",
    "        if(time_steps< EXPLORE_TIMESTEPS): \n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)\n",
    "        else:\n",
    "            actions = maddpgagent.act(states)\n",
    "\n",
    "        # Pass the actions to the environment\n",
    "        env_info = env.step(actions)[brain_name]  \n",
    "        # Get the next states\n",
    "        next_states = env_info.vector_observations\n",
    "        # Get the rewards\n",
    "        rewards = env_info.rewards         \n",
    "        # Check for Completion\n",
    "        dones = env_info.local_done  \n",
    "        # Store the tuples into memory\n",
    "        tennis.step(states, actions, rewards, next_states, dones)    \n",
    "        # Move to next state\n",
    "        states = next_states\n",
    "        time_steps+=1\n",
    "        score += rewards  \n",
    "\n",
    "        # If any agent fails or they play for 1000 timesteps environment will restart\n",
    "        if any(dones):                                 \n",
    "            break\n",
    "\n",
    "    score_max = np.max(score)\n",
    "    scores.append(score_max)\n",
    "    scores_deque.append(score_max)\n",
    "    average_score = np.mean(scores_deque)\n",
    "    average_scores.append(average_score)\n",
    "\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(episode, np.mean(scores_deque)), end=\"\")  \n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage score: {:.3f}'.format(episode , average_score))\n",
    "\n",
    "    if average_score >= 0.5:\n",
    "        maddpgagent.save_weights()\n",
    "        print(\"\\rSolved in episode: {} \\tAverage score: {:.3f}\".format(episode , average_score))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1, len(average_scores)+1), average_scores)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Scores into arrays\n",
    "np.save(\"DDPGresult_scores_Exploration\", scores)\n",
    "np.save(\"DDPGresult_Exploration\", (np.array(average_scores_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):                                    \n",
    "    env_info = env.reset(train_mode=False)[brain_name]     \n",
    "    states = env_info.vector_observations               \n",
    "    score = np.zeros(num_agents)\n",
    "    tennis.reset()\n",
    "    while True:\n",
    "        actions = tennis.act(states)\n",
    "        env_info = env.step(actions)[brain_name]            \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards         \n",
    "        dones = env_info.local_done                         \n",
    "        tennis.step(states, actions, rewards, next_states, dones)        \n",
    "        states = next_states\n",
    "        score += rewards  \n",
    "\n",
    "        if any(dones):                                 \n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
